{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Denoising Auto Encoder with TensorFlow\n",
    "--------- \n",
    "Nikolas Wolfe<br>\n",
    "Deep Learning Seminar<br>\n",
    "December 20th, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Download PASCAL VOC2007 dataset from VOC2007 website. This dataset includes training, validation and testing sets. You can fnd the images inside JPEGImages folder and the labels inside the ImageSets/Main folder.\n",
    "\n",
    "I downloaded the VOC2007 dataset, and I have placed the train/test data in two folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -l train/VOC2007/\n",
    "ls -l test/VOC2007/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Together there are 2501 training images, 2510 validation images and 4952 testing images. Train a Denosing Autoencoder with different noise levels using the training set and visualize the learned filters as in here. Here you can use the whole image (re-size to a fixed size, e.g., 256x256) or sub-sampled small patches with a fixed size (e.g., 30x30). Comparing both of them will give you extra credits.\n",
    "### Image Input Types to Test:\n",
    "* 256 x 256 Resizing\n",
    "* Convolutional sub-samples (30 x 30)\n",
    "\n",
    "First, some imports and constants to make this easier down the line:\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------- TENSORFLOW --------- #\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sys, os, numpy\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------ General ------------------- #\n",
    "IMG_DIR = 'VOC2007' + os.sep + 'JPEGImages' + os.sep\n",
    "LBL_DIR = 'VOC2007' + os.sep + 'ImageSets' + os.sep + 'Main' + os.sep\n",
    "\n",
    "# ------------------ Training ------------------ #\n",
    "TRAIN_DIR = 'train' + os.sep + IMG_DIR\n",
    "TRAIN_LABELS = 'train' + os.sep + LBL_DIR\n",
    "\n",
    "# ------------------ Testing ------------------- # \n",
    "TEST_DIR = 'test' + os.sep + IMG_DIR\n",
    "TEST_LABELS = 'test' + os.sep + LBL_DIR\n",
    "\n",
    "# ------------------ Misc ------------------- # \n",
    "NUM_CLASSES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(TRAIN_DIR)\n",
    "print(TRAIN_LABELS)\n",
    "print(TEST_DIR)\n",
    "print(TEST_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels for the images are stored in the 'Main' directory, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls train/VOC2007/ImageSets/Main/ | head -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in these files are in the following form, where the 1st column corresponds to the jpeg file and the 2nd column corresponds to whether the label is present, as shown in the output below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "tail train/VOC2007/ImageSets/Main/bicycle_train.txt | head -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to verify this, we can look at somet of these files from the 'JPEGImages' folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "x = Image(filename=TRAIN_DIR + '009926.jpg', width=256, height=256) # this IS a bicycle\n",
    "y = Image(filename=TRAIN_DIR + '009940.jpg', width=256, height=256) # this is NOT a bicycle\n",
    "display(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the Data\n",
    "We first need to import the dataset and get it into the appropriate data structures which are used in TensorFlow. In this case I will not separate the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generic read data from our file structure...\n",
    "def get_data(DIR, LABELS, key):\n",
    "    imgs = {}\n",
    "    labels = defaultdict(list)\n",
    "    for root, dirs, files in os.walk(LABELS, topdown=False):\n",
    "        for f in files: \n",
    "            if '_' + key in f: \n",
    "                label = f.split('_')[0] \n",
    "                f = os.path.abspath(LABELS + f)\n",
    "                pos_ex = [l.split()[0] for l in open(f).readlines() if l.split()[-1] == '1']\n",
    "                for img in pos_ex:\n",
    "                    labels[label].append(img)\n",
    "                    imgs[img] = os.path.abspath(DIR + img + '.jpg')\n",
    "    return imgs, labels\n",
    "\n",
    "# Dig through for the training data...\n",
    "def get_train_data(): return get_data(TRAIN_DIR, TRAIN_LABELS, \"trainval.txt\")        \n",
    "\n",
    "# Dig through for the testing data...\n",
    "def get_test_data(): return get_data(TEST_DIR, TEST_LABELS, \"test.txt\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in train and test data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --------- TRAIN & TEST SETS --------- #\n",
    "train_imgs, train_labels = get_train_data()\n",
    "test_imgs, test_labels = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in classification later on, we're going to find it useful to have 20-dimensional 1-hot bit vectors to represent our 20 PASCAL classification labels. So we'll go ahead and create those representations here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_one_hot_label_list(label_dict):\n",
    "    keys = OrderedDict(sorted(label_dict.items())).keys()\n",
    "    one_hot_lookup = {}\n",
    "    for i in range(len(keys)):\n",
    "        one_hot = numpy.zeros(NUM_CLASSES)\n",
    "        one_hot[i] = 1\n",
    "        one_hot_lookup[keys[i]] = one_hot\n",
    "    one_hot_list = []\n",
    "    for label in label_dict.keys():\n",
    "        vals = label_dict[label]\n",
    "        for v in vals:\n",
    "            one_hot_list.append((v, one_hot_lookup[label]))\n",
    "    return one_hot_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a mapping between the image keys and the one-hot labels. We add a duplicate image key entry to the list if the image contains more than one label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels_one_hot = get_one_hot_label_list(train_labels)\n",
    "test_labels_one_hot = get_one_hot_label_list(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to read in the images and create TensorFlow tensors with them... In this function we will associate the one-hot dictionaries with the files their keys point to. As per the requirements of the first part of this assignment, we will also call an operation to resize the images to 256 x 256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_img_data(one_hot_dict, file_dict):\n",
    "    labels = []\n",
    "    files = []\n",
    "    img_files = []\n",
    "    reader = tf.WholeFileReader()\n",
    "    print('Assembling files...')\n",
    "    for (img, label_vec) in one_hot_dict:\n",
    "        files.append(file_dict[img])\n",
    "        labels.append(label_vec)\n",
    "    queue = tf.train.string_input_producer(files)\n",
    "    jpg_key, jpg_img = reader.read(queue)\n",
    "    jpg_img = tf.image.decode_jpeg(jpg_img)\n",
    "    jpg_tensor = tf.convert_to_tensor(jpg_img)\n",
    "    jpg_tensor_resized = tf.image.resize_images(jpg_tensor, 256, 256)\n",
    "    init = tf.initialize_all_variables()\n",
    "    # Run session...\n",
    "    print('Starting tensorflow session...')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(len(files)):\n",
    "            jpg = jpg_tensor_resized.eval()\n",
    "            img_files.append(jpg)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    # build numpy arrays\n",
    "    print('Reconstructing arrays...')\n",
    "    img_files = numpy.asarray(img_files)\n",
    "    labels = numpy.asarray(labels)\n",
    "    print('Done!')\n",
    "    return img_files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_img_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-951138fcc0fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mread_img_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'read_img_data' is not defined"
     ]
    }
   ],
   "source": [
    "read_img_data(train_labels_one_hot, train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building An AutoEncoder\n",
    "The first step on this journey is to download and install a few packages we will need. I will be building this project using Google <a href=\"https://www.tensorflow.org/\">TensorFlow</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "An Auto-encoder w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Deep Auto-Encoder implementation\n",
    "\t\n",
    "\tAn auto-encoder works as follows:\n",
    "\tData of dimension k is reduced to a lower dimension j using a matrix multiplication:\n",
    "\tsoftmax(W*x + b)  = x'\n",
    "\t\n",
    "\twhere W is matrix from R^k --> R^j\n",
    "\tA reconstruction matrix W' maps back from R^j --> R^k\n",
    "\tso our reconstruction function is softmax'(W' * x' + b') \n",
    "\tNow the point of the auto-encoder is to create a reduction matrix (values for W, b) \n",
    "\tthat is \"good\" at reconstructing  the original data. \n",
    "\tThus we want to minimize  ||softmax'(W' * (softmax(W *x+ b)) + b')  - x||\n",
    "\tA deep auto-encoder is nothing more than stacking successive layers of these reductions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Given the PASCAL dataset from Part 1, train a Stacked Denosing Autoencoder\n",
    "as in here using the training set and use it as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A fixed network to extract features. Train RBF SVMs using the extracted features from the training set. Again, here you can use a small network trained with sub-sampled inputs and average the output of the network to get image-level features when you train the SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A pre-trained network to initialize a feed-forward network and finne-tune the feed-forward network using the training set. Here you will have to use the whole re-sized images as the inputs. Given the fact that an image may have multiple labels in this dataset, you may not want to use a softmax layer as your output layer. Explore the network structure and SVM parameters to achieve the best validation accuracy (mAP for SVM) you can get. Report the best network structure, SVM parameters the corresponding testing MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
